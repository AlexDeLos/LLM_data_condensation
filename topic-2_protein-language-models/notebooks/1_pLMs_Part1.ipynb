{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad5bd5f0-34c0-4ffd-9256-c0ffcc2eb8bd",
   "metadata": {},
   "source": [
    "# Protein Language Models Part 1\n",
    "### Learning objectives: \n",
    "- Load a pre-trained pLM\n",
    "- Investigate the internal representation of tokens\n",
    "\n",
    "Optional\n",
    "- Fine-Tune a pLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77a276b-1a57-42a5-a491-c25575b8f825",
   "metadata": {},
   "source": [
    "------------------------------------------------\n",
    "## 1. Investigate protein representation in the pLM ProtBERT\n",
    "(Adapted from [DeepChem Tutorials](https://github.com/deepchem/deepchem/blob/master/examples/tutorials/ProteinLM_Tutorial0.ipynb), check the original notebook for more information.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef473db-ba2d-4c8c-8a18-723d87171dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from transformers import BertForMaskedLM, BertTokenizer, pipeline\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35948bba-b323-470d-a416-2201c3b2fb6d",
   "metadata": {},
   "source": [
    "### Proteins of interest to be investigated\n",
    "Hemoglobin is the protein responsible for transporting oxygen from the lungs to all the cells of our body via red blood cells. Hemoglobin is a great protein to interrogate the behaviors of protein language models as it is highly conserved in certain regions across species, and also slightly variable in other places. What would we expect the distribution over amino acids to look like if we mask out a highly conserved region? What about a highly diverse region? Let's find out.\n",
    "\n",
    "Hemoglobin Sequence Homology across closely related mammals (from [Ali et. al](https://www.nature.com/articles/s41598-019-50619-w)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75253b20-067c-4fc5-9092-feddaff1b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hemoglobin_beta = {\n",
    "'human':\n",
    "\"MVHLTPEEKSAVTALWGKVNVDEVGGEALGRLLVVYPWTQRFFESFGDLSTPDAVMGNPKVKAHGKKVLGAFSDGLAHLDNLKGTFATLSELHCDKLHVDPENFRLLGNVLVCVLAHHFGKEFTPPVQAAYQKVVAGVANALAHKYH\",\n",
    "'chimpanzee':\n",
    "\"MVHLTPEEKSAVTALWGKVNVDEVGGEALGRLLVVYPWTORFFESFGDLSTPDAVMGNPKVKAHGKKVLGAFSDGLAHLDNLKGTFATLSELHCDKLHVDPENFRLLGNVLVCVLAHHFGKEFTPPVQAAYQKVVAGVANALAHKYH\",\n",
    "'camel':\n",
    "\"MVHLSGDEKNAVHGLWSKVKVDEVGGEALGRLLVVYPWTRRFFESFGDLSTADAVMNNPKVKAHGSKVLNSFGDGLNHLDNLKGTYAKLSELHCDKLHVDPENFRLLGNVLVVVLARHFGKEFTPDKQAAYQKVVAGVANALAHRYH\",\n",
    "'rabbit':\n",
    "\"MVHLSSEEKSAVTALWGKVNVEEVGGEALGRLLVVYPWTQRFFESFGDLSSANAVMNNPKVKAHGKKVLAAFSEGLSHLDNLKGTFAKLSELHCDKLHVDPENFRLLGNVLVIVLSHHFGKEFTPQVQAAYQKVVAGVANALAHKYH\",\n",
    "'pig':\n",
    "\"MVHLSAEEKEAVLGLWGKVNVDEVGGEALGRLLVVYPWTQRFFESFGDLSNADAVMGNPKVKAHGKKVLQSFSDGLKHLDNLKGTFAKLSELHCDQLHVDPENFRLLGNVIVVVLARRLGHDFNPNVQAAFQKVVAGVANALAHKYH\",\n",
    "'horse':\n",
    "\"*VQLSGEEKAAVLALWDKVNEEEVGGEALGRLLVVYPWTQRFFDSFGDLSNPGAVMGNPKVKAHGKKVLHSFGEGVHHLDNLKGTFAALSELHCDKLHVDPENFRLLGNVLVVVLARHFGKDFTPELQASYQKVVAGVANALAHKYH\",\n",
    "'bovine':\n",
    "\"M**LTAEEKAAVTAFWGKVKVDEVGGEALGRLLVVYPWTQRFFESFGDLSTADAVMNNPKVKAHGKKVLDSFSNGMKHLDDLKGTFAALSELHCDKLHVDPENFKLLGNVLVVVLARNFGKEFTPVLQADFQKVVAGVANALAHRYH\",\n",
    "'sheep':\n",
    "\"M**LTAEEKAAVTGFWGKVKVDEVGAEALGRLLVVYPWTQRFFEHFGDLSNADAVMNNPKVKAHGKKVLDSFSNGMKHLDDLKGTFAQLSELHCDKLHVDPENFRLLGNVLVVVLARHHGNEFTPVLQADFQKVVAGVANALAHKYH\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c16b7d-12a4-4613-af66-e5052da72e44",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "[ProtBERT](https://arxiv.org/abs/2007.06225) is a protein language model based on the BERT model.\n",
    "Load ProtBERT, use the [pre-trained Uniref100 Model](https://huggingface.co/Rostlab/prot_bert). Also load the tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef0b9f-4821-457c-a5e1-12e5281cd21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "model = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert\", weights_only=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cfe126-740e-4c88-94af-6319cfa554a8",
   "metadata": {},
   "source": [
    "### See how the model recovers masked positions in Hemoglobin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8269966-c072-4821-9508-7c8be2c00851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the F8 Histidine of Hemoglobin B Subunit\n",
    "human_heme = list(hemoglobin_beta['human'])\n",
    "human_heme[92] = \"[MASK]\"\n",
    "masked_heme = ' '.join(human_heme)\n",
    "print(masked_heme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32758e6e-488a-42e3-ae40-473655256166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise the sequence and pass it through the model\n",
    "tokenized_sequence = tokenizer(masked_heme, return_tensors='pt')\n",
    "tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7375b3-e23b-488c-8367-5ddcb9461dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise the sequence and pass it through the model\n",
    "model_outs = model(**tokenized_sequence)\n",
    "model_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0433cee-b7f0-4c24-9f70-7167ef34e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the logits\n",
    "logits = model_outs.logits.squeeze()[1:-1] # Ignore SOS and EOS special tokens\n",
    "print(logits.shape)\n",
    "softmaxed = F.softmax(logits, dim=1).detach().numpy() # Softmax to normalize the logits to sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f1d70-9faf-4439-986b-62967b261065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode the Logits Using Greedy Decoding (Max Probability at Each Timestep)\n",
    "decoded_outputs = tokenizer.batch_decode(softmaxed.argmax(axis=1))\n",
    "decoded_sequence = ''.join(decoded_outputs)\n",
    "print(decoded_sequence)\n",
    "print(f'The filled-in masked sequence is: {decoded_sequence[92]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e87986a-87d7-46c8-901a-f9fa1315321b",
   "metadata": {},
   "source": [
    "**Sanity Check:** Looks like the pLM ProtBERT was able to recapitulate the correct amino acid at that position. But how confident was the model? Let's visualize the distribution at that position and see what other amino acids the  model was choosing between.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e3772-761f-4a25-b543-c8f9f1171654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the Token Distribution at the F8 Histidine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(tokenizer.get_vocab().keys(), softmaxed[92])\n",
    "plt.ylabel('Normalized Probability')\n",
    "plt.xlabel('Model Vocabulary')\n",
    "plt.title('Target Distribution at the F8 Histidine')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca36c48f-f5b4-4b40-b7df-a8460654f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the Logits Map Across All Positions\n",
    "plt.figure(figsize=(10,16))\n",
    "sns.heatmap(softmaxed, xticklabels=tokenizer.get_vocab())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32dfb3-f1b9-40f3-a71b-c1176c698297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a Low Confidence Region\n",
    "\n",
    "plt.bar(tokenizer.get_vocab().keys(), softmaxed[87])\n",
    "plt.ylabel('Normalized Probability')\n",
    "plt.xlabel('Model Vocabulary')\n",
    "plt.title('Target Distribution at Position 87')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2169d-0115-4e52-bb82-47a6bc86ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for animal in hemoglobin_beta:\n",
    "    print(f'{animal} has residue {hemoglobin_beta[animal][87]} at position 87')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f8377-0eef-40d8-84e8-3f132e1b30bc",
   "metadata": {},
   "source": [
    "------------------------------------------------\n",
    "## Optional: To run the second part we need Python <3.12 and the newest DeepChem version. Skip if setting up the env is too tidious.\n",
    "## 2. Fine-Tune ProtBERT for water solubility\n",
    "(Adapted from [DeepChem Tutorials](https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Introduction_to_ProtBERT.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9114f-b37f-4b02-a3e9-6b03807294ef",
   "metadata": {},
   "source": [
    "Use the [DeepLoc](https://academic.oup.com/bioinformatics/article/33/21/3387/3931857?login=false) dataset for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b930357-8e3e-4ffe-a325-29f5de032070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "from deepchem.models.torch_models import ProtBERT\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from urllib.request import urlopen\n",
    "from rich.progress import Progress, TransferSpeedColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e191a-b9e7-4ced-8ba3-d06070574c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## datasets for fine-tuning\n",
    "URL_test = \"https://deepchemdata.s3.us-west-1.amazonaws.com/datasets/DeepLoc_test.csv\"\n",
    "URL_train = \"https://deepchemdata.s3.us-west-1.amazonaws.com/datasets/DeepLoc_train.csv\"\n",
    "out_test = './datasets/DeepLoc_test.csv'\n",
    "out_train = \"./datasets/DeepLoc_train.csv\" \n",
    "os.makedirs('datasets', exist_ok=True)\n",
    "\n",
    "print(URL_test)\n",
    "with Progress(*Progress.get_default_columns(), TransferSpeedColumn()) as progress:\n",
    "    with urlopen(URL_test) as res:\n",
    "        length = int(res.headers[\"Content-Length\"])\n",
    "        with progress.wrap_file(res, total=length) as src, open(out_test, \"wb\") as dest:\n",
    "            shutil.copyfileobj(src, dest)\n",
    "print(URL_train)\n",
    "with Progress(*Progress.get_default_columns(), TransferSpeedColumn()) as progress:\n",
    "    with urlopen(URL_train) as res:\n",
    "        length = int(res.headers[\"Content-Length\"])\n",
    "        with progress.wrap_file(res, total=length) as src, open(out_train, \"wb\") as dest:\n",
    "            shutil.copyfileobj(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f9ab5-da0a-4606-94c0-44498902fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo purpose we choose a subset of the orginal data\n",
    "train_df = pd.read_csv(out_train)\n",
    "string_lengths = train_df[\"protein\"].apply(len)\n",
    "filtered_train_df = train_df[string_lengths < 200].sample(5000)\n",
    "filtered_train_df.to_csv(\"./datasets/DeepLoc_train_5000.csv\",index=False)\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(out_test)\n",
    "string_lengths = test_df[\"protein\"].apply(len)\n",
    "filtered_test_df = test_df[string_lengths < 200].sample(1000)\n",
    "filtered_test_df.to_csv(\"./datasets/DeepLoc_test_1000.csv\",index=False)\n",
    "\n",
    "filtered_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40827a3c-e1d8-4fd7-b22e-06ca0a7e72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurise the \n",
    "featurizer = dc.feat.DummyFeaturizer()\n",
    "tasks = [\"water soluble\"]\n",
    "loader = dc.data.CSVLoader(tasks=tasks,\n",
    "                            feature_field=\"protein\",\n",
    "                            featurizer=featurizer)\n",
    "deeploc_train_dataset = loader.create_dataset(\"./datasets/DeepLoc_train_5000.csv\")\n",
    "deeploc_test_dataset  = loader.create_dataset(\"./datasets/DeepLoc_test_1000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6148bbbc-b204-4292-ab66-66154827eeaf",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "Load [ProtBERT](https://academic.oup.com/bioinformatics/article/38/8/2102/6502274?login=false), use the [pre-trained Uniref100 Model](https://huggingface.co/Rostlab/prot_bert). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f548389-8396-46b5-bc2c-457cca9ceb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir for finetuning\n",
    "finetune_model_dir = \"finetuning/\"\n",
    "\n",
    "# Network for custom classfication task\n",
    "custom_network = nn.Sequential(nn.Linear(1024, 512),\n",
    "                               nn.ReLU(), nn.Linear(512, 256),\n",
    "                               nn.ReLU(), nn.Linear(256, 2)) \n",
    "\n",
    "# ProtBERT model that can be used for fine-tuning for a downstream task\n",
    "ProtBERTmodel_for_classification = ProtBERT(task='classification',\n",
    "                                            model_path=\"Rostlab/prot_bert\",\n",
    "                                            n_tasks=1,\n",
    "                                            cls_name=\"custom\",\n",
    "                                            classifier_net=custom_network,\n",
    "                                            n_classes=2,\n",
    "                                            model_dir=finetune_model_dir,\n",
    "                                            batch_size=32,\n",
    "                                            learning_rate=1e-5,\n",
    "                                            log_frequency = 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e48bef-1592-43ca-a6c5-04cad02c4029",
   "metadata": {},
   "source": [
    "### Fine-tune the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b84004-9de4-44b8-b92d-6a6bdfbbb7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze underlying ProtBERT and only train the classfier head\n",
    "for param in ProtBERTmodel_for_classification.model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# track the loss\n",
    "all_losses = []\n",
    "loss = ProtBERTmodel_for_classification.fit(deeploc_train_dataset, nb_epoch=1,all_losses = all_losses)\n",
    "\n",
    "# Plot training loss\n",
    "batches = list(range(5, 5 * (len(all_losses) + 1), 5))\n",
    "plt.plot(batches, all_losses, linestyle='-', color='b')\n",
    "plt.title('Training Loss over Batches')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d451a7-6bb9-4635-9ce0-743163a63457",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "Use the deepchem metrics (e.g. accuracy score) to evaluate your final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79caee-6f0f-48b4-88c8-246d3ef56c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)\n",
    "eval_score = ProtBERTmodel_for_classification.evaluate(deeploc_test_dataset, [classification_metric],n_classes=2)\n",
    "eval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec187e-8102-48b4-a549-6533fbe34342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
